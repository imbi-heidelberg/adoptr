---
title: "Using other endpoints"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using other endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse   = TRUE,
  comment    = "#>",
  fig.width  = 7,
  fig.height = 5
)
```

```{r setup}
library(adoptr)
```

We use the following notation:

- $X_i^T$ denotes the endpoint of recruit $i$ in the treatment arm
- $X_i^C$ denotes the endpoint of recruit $i$ in the control arm
- $n$ denotes the group-wise sample size



In the simplest cases, only normal-distributed endpoints are considered and depending on the sample size, either a z-test or a t-test can be conducted. 
However, a lot of trials require binary or time-to-event endpoints, e.g. when it is only important to know whether a recruit is alive or how long a patient lived. Of course, binary endpoints or time-to-event data are not necessarily connected to deaths in general, so in the following, we only say that an "event occurred". 

# Binary endpoints

When dealing with binary endpoints, we know that the endpoints follow a Bernoulli distribution, so $X_i^T \sim Bin(1,p_T)$ and $X_i^C\sim Bin(1,p_C)$. We now want to compare the probability of an event in the treatment group $p_T$ with a fixed value (single-arm trial) or with the probability of an event in the control group $p_C$ (two-armed trial).  Thus, assuming that larger probabilities are favorable, we have

$$
H_0: p_T \leq p_C\quad \text{vs.} \quad H_1:p_C\leq p_T.
$$

We want to have a test statistic that asymptotically follows a normal distribution, so we use 
$$
U=\sqrt{\frac{n}{2}}\frac{\hat{p}_T-\hat{p}_C}{\sqrt{\hat{p}_0(1-\hat{p}_0)}},
$$

where $\hat{p}_T=\frac{1}{n}\sum_i^{n}X_i^T$, $\hat{p}_C=\frac{1}{n}\sum_i^{n}X_i^C$ are estimators for $p_T$ and $p_C$. Furthermore, $\hat{p}_0=\frac{\frac{1}{n}\sum_i^{n}X_i^T+\frac{1}{n}\sum_i^{n}X_i^C}{2n}$ is the estimator for the pooled rate $p_0=\frac{p_T+p_C}{2}$. 

The outcome of $U$ is then compared to $c_f$ and $c_e$, the first stage boundaries. If $U\in [c_f,c_e]$, we continue the trial and compute a new value for $U_2$ in the second stage, where we reject the null if $U_2>c_2$.

## Asymptotic distribution of the test statistic

We begin with the difference $\hat{p}_T-\hat{p}_C$. Using the de Moivre-Laplace theorem, we get that 
$$
\frac{n\hat{p}_T-np_T}{\sqrt{n}} \overset{d}{\to} \mathcal{N}(0,p_T(1-p_T)).
$$
After defining $\sigma_A^2=p_T(1-p_T)+p_C(1-p_C)$, we obtain
$$
\frac{n\hat{p}_T-np_T}{\sqrt{n}}-\frac{n\hat{p}_C-np_C}{\sqrt{n}}=\sqrt{n}(\hat{p}_T-\hat{p}_C-(p_T-p_C))\overset{d}{\to}\mathcal{N}(0,\sigma_A^2),
$$
so it follows
$$
\sqrt{n}(\hat{p}_T-\hat{p}_C)\overset{d}{\to}\mathcal{N}(\sqrt{n}(p_T-p_c),\sigma_A^2).
$$
Applying the continuous mapping theorem, it results that $\hat{\sigma}_0:=\sqrt{2\hat{p}_0(1-\hat{p}_0)} \overset{P}{\to}\sqrt{2p_0(1-p_0)}:=\sigma_0$, so by Slutzky's theorem, we get
$$
\frac{\sqrt{n}(\hat{p}_T-\hat{p}_C)}{\sigma_0}=\sqrt{\frac{n}{2}}\frac{\hat{p}_T-\hat{p}_C}{\sqrt{\hat{p}_0(1-\hat{p}_0)}}\overset{d}{\to}\mathcal{N}(\sqrt{n}\frac{p_T-p_C}{\sigma_0},\frac{\sigma_A^2}{\sigma_0^2}).
$$
Hence, the following statement approximately holds:
$$
U \sim \mathcal{N}(\sqrt{n}\frac{p_T-p_C}{\sigma_0},\frac{\sigma_A^2}{\sigma_0^2}).
$$

Note that under the null hypothesis, $\sigma_A^2=\sigma_0^2$ and $p_T=p_C$. Thus, $U\sim \mathcal{N}(0,1)$ under $H_0$.

## **adoptr** and binomial endpoints

### Implementation details

Due to the fact that the test statistic $U$ explicitly depends on the outcome of $p_T$ and $p_C$ and not only their difference $\theta=p_T-p_C$, it is necessary to fix the rate in the control group $p_C$. Thus, by knowing $\theta$, **adoptr** implicitly computes $p_T=p_C+\theta$. 
Unfortunately, this means that $p_C$ is the same under the null as well as the alternative. This aspect could lead to limitations, but if the control group is assumed to be a placebo group, it makes sense to assume that $p_C$ is constant under $H_0$ and $H_1$.

### Example

Let $p_C=0.3$ be fixed. Furthermore, we assume to have  two-armed trial.

```{r}
datadist <- Binomial(0.3, two_armed = TRUE)
```

Let us furthermore assume that the we have a normal prior on $\theta$ with expectation $\mu=0.2$ and standard deviation $\sigma=0.2$, which was truncated to the interval $(-.029,0.69)$. It is necessary to use a truncation to ensure that $p_T \in (0,1)$. 

```{r}
H_0        <- PointMassPrior(.0, 1)
prior      <- ContinuousPrior(function(x) 1/(pnorm(0.69,0.2,0.2)-pnorm(-0.29,0.2,0.2))*dnorm(x,0.2,0.2),
                              support = c(-0.29,0.69),
                              tighten_support = TRUE)
```

We require a maximal type one error of $\alpha\leq 0.025$ and a minimum expected power of $\mathbb{E}[1-\beta]\geq 0.8$. 

```{r}
alpha      <- 0.025
min_epower <- 0.8
toer_cnstr <- Power(datadist, H_0) <= alpha
epow_cnstr <- Power(datadist, condition(prior, c(0.0,0.69))) >= min_epower

```

We choose our design parameters such that the expected sample size is minimized. In order to find an initial design, we use the in-built function \texttt{get_initial_design}.

```{r}
ess <- ExpectedSampleSize(datadist,prior)

init <- get_initial_design(0.2,0.025,0.2)

opt_design <- minimize(ess,subject_to(toer_cnstr,epow_cnstr),initial_design = init, check_constraints = TRUE)

plot(opt_design$design)
```

# Time-to-event endpoints

Another quite common endpoint are so called time-to-event endpoints occurring in survival analysis. In the following, let $\theta := \frac{\lambda_C(t)}{\lambda_T(t)}$ be the ratio of the hazard functions in either group. We assume that $\theta$ is constant.
Assuming that a lower hazard function is favorable, the resulting hypotheses are

$$
H_0: \theta\leq 1 \quad \text{vs.} \quad H_1: \theta >1.
$$

Let $1,\dots,J$ be the distinct times of observed events in either group and let $n_{T,j}, n_{C,j}$ be the number of subjects, who neither had an event nor have been censored. Additionally, $O_{T,j},O_{C,j}$ are the observed number of events at time $j$. Define $n_j:=n_{T,j}+n_{C,j}$ and $O_j:= O_{T,j}+O_{C,j}$. Under the null, the hazard functions of the groups are equal. Thus, for $i \in \{T,C\}$, $O_{j}$ can be regarded as the number of events in a draw of size $n_{i,j}$, where the population has size $n_j$. This is exactly the definition of the hypergeometric distribution, so we know that $O_{i,j}\sim h(n_j,O_j,n_{i,j})$. This distribution has an expected value of $E_{i,j}:=\mathbb{E}[O_{i,j}]=n_{i,j}\frac{O_j}{n_j}$ and variance $V_{i,j}:=\text{Var}(O_{i,j})=E_{i,j}(\frac{n_j-O_j}{n_j})(\frac{n_j-n_{i,j}}{n_j-1})$. The log-rank test statistic is given by

$$
L=L_T:=\frac{\sum_{j=1}^J O_{T,j}-E_{T,j}}{\sqrt{\sum_{j=1}^J V_{T,j}}}
$$
Note that it does not matter whether we consider $L_T$ or $L_C$, both of them yield the same results, so we abbreviate $L_T$ to $L$.
By the central limit theorem, it is easy to see that $L \overset{d}{\to} \mathcal{N}(0,1)$. Under the alternative, it can be shown that $L \sim \mathcal{N}(\text{log}(\theta)\frac{1}{2}\sqrt{n_E},1)$ approximately, where $n_E$ is the number of events in either group. If $d$ is the combined probability of events (also known as event rate) in both arms, $n_E$ can be replaced by $2nd$ (where $n$ is, like in the whole chapter, the number of recruits per group). Thus, by estimating $d$ via a suitable estimator $\hat{d}$, we get an estimation of the total number of recruits for the study. 
In **adoptr**, the parameter $d$ needs to be pre-specified in order to obtain a one-parametric distribution (the other parameter is $\theta$), so $d$ is assumed to be fixed during the trial. This is similar to the case of binary endpoints, where the response rate in the control group $r_C$ had to be a constant value. Since the number of events is the main interest during a survival study, the need of a constant $d$ is not a strong limitation.

## Survival analysis and adapitve designs

### Notation

We expand the previous notation by an index $k\in \{1,2\}$, denoting the current stage. We observe $J_1$ events until the interim analysis, and we observe $J_2$ events until the final analysis. Note that $J_2$ is meant to be cumulative. Furthermore, let $O_{i,j,k}$ be the number of observed events in arm $i\in \{T,C\}$ at time $j \in \{1,\dots,J_k\}$ in stage $k \in \{1,2\}$. Like in the previous section, let, $E_{i,j,k}= \mathbb{E}[O_{i,j,k}]=n_{i,j,k}\frac{O_{j,k}}{n_{j,k}}$, where $n_{i,j,k}$ is the number of patients that have not had an event until the end of stage $k$, $O_{j,k}=O_{T,j,k}+O_{C,j,k}$ and $n_{j,k}=n_{T,j,k}+n_{C,j,k}$. In the same way, we can define $V_{i,j,k}$ like in the previous section. In this setting, we now have two test statistics $L_k$, $k \in \{1,2\}$, that can be calculated the same way like described above:

$$
L_k=\frac{\sum_{j=1}^{J_k} O_{T,j,k}-E_{T,j,k}}{\sqrt{\sum_{j=1}^{J_k} V_{T,j,k}}}
$$

### Over-running and solutions

In survival analysis, it is usual to have a certain follow-up time after the accrual period in order to evaluate the number of events and when they occurred. Thus, when conducting the interim analysis in an adaptive design, some patients may not have had an event yet, even though those could lead to an opposite decision than the patients available at interim do. This issue is known as "over-running". Additionally, these recruits that have not had an event till the interim analysis provide information for both stages: in the first stage, we know that these subjects survived, and in the second stage, they may die (or even survive until the end of the trial). Thus, the test statistics $L_{T,1}$ from the interim analysis and $L_{T,2}$ from the final analysis are not independent, even though this is an important assumption in **adoptr** in order to calculate optimal designs. 

#### Solution 1: Independent Increments

It is easy to verify that 

$$
Z_2:= \frac{\sqrt{J_2}L_2-\sqrt{J_1}L_1}{\sqrt{J_2-J_1}}
$$
is still approximately standard normal under the null. 
