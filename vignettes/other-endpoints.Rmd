---
title: "Adaptations to the testing procedure for non-normal endpoints"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using other endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse   = TRUE,
  comment    = "#>",
  fig.width  = 7,
  fig.height = 5
)
```


```{r setup, include=FALSE}
library(adoptr)
```

Currently, `adoptr` supports the optimization of adaptive two-stage designs with
either normally or t-distributed test statistics. More specifically,
it is required that the first stage test statistic, and the
distribution of the second stage test-statistic
conditional on the first-stage test statistic, is either normal or t-distributed.

When individual observations are normally distributed, stage-wise
Z- or T-test statistics fulfill these criteria.
In practice, however, there are many situations where one cannot reasonably
assume that individual observations are sampled from a normal distribution.
It is common for trials to be conducted for binary or time-to-event
endpoints, e.g. if one is interested in investigating the impact of a drug
on mortality. One way to deal with this situation is by transforming the
data to a test-statistic which is approximately normal. In the following, we will
explain how to do so for binary and for time-to-event endpoints.

We will be using the following notation:

- $X_i^T$ will denote an observation of recruit $i$ in the treatment arm
- $X_i^C$ will denote the observation of recruit $i$ in the control arm
- $n$ will denote the group-wise sample size.

# Binary endpoints
Binary endpoints are endpoints where individual observations follow a Bernoulli
distribution, i.e. $X_i^T \sim Bin(1,p_T)$ and $X_i^C\sim Bin(1,p_C)$. Our goal
is to compare the probability $p_T$ of an event in the treatment group
with a fixed value (single-arm trial) or with the probability $p_C$ of an event in the
control group (two-armed trial). Thus, assuming that larger probabilities are favorable, we have

$$
H_0: p_T \leq p_C\quad \text{vs.} \quad H_1:p_T > p_C.
$$

To test this hypothesis, one could use the test statistic 
$$
U=\sqrt{\frac{n}{2}}\frac{\hat{p}_T-\hat{p}_C}{\sqrt{\hat{p}_0(1-\hat{p}_0)}},
$$

where $\hat{p}_T=\frac{1}{n}\sum_i^{n}X_i^T$ and $\hat{p}_C=\frac{1}{n}\sum_i^{n}X_i^C$ are the
maximum likelihood estimators of $p_T$ and $p_C$ and where
$\hat{p}_0=\frac{\frac{1}{n}\sum_i^{n}X_i^T+\frac{1}{n}\sum_i^{n}X_i^C}{2}$.

The outcome of $U$ is then compared to $c_f$ and $c_e$, the first stage boundaries.
If $U\in [c_f,c_e]$, we continue the trial and compute a new value for $U_2$ in
the second stage, where we reject the null if $U_2>c_2$.

It is a well-known fact that this test statistic is asymptotically normal, and we will
give a proof of this in the next section.

## Asymptotic distribution of the test statistic
We begin with the difference $\hat{p}_T-\hat{p}_C$. Using the de Moivre-Laplace theorem, we get that 
$$
\frac{n\hat{p}_T-np_T}{\sqrt{n}} \overset{d}{\to} \mathcal{N}(0,p_T(1-p_T)).
$$
After defining $\sigma_A^2=p_T(1-p_T)+p_C(1-p_C)$, we obtain
$$
\frac{n\hat{p}_T-np_T}{\sqrt{n}}-\frac{n\hat{p}_C-np_C}{\sqrt{n}}=\sqrt{n}(\hat{p}_T-\hat{p}_C-(p_T-p_C))\overset{d}{\to}\mathcal{N}(0,\sigma_A^2),
$$
so it follows
$$
\sqrt{n}(\hat{p}_T-\hat{p}_C)\overset{d}{\to}\mathcal{N}(\sqrt{n}(p_T-p_c),\sigma_A^2).
$$
Applying the continuous mapping theorem, it results that $\hat{\sigma}_0:=\sqrt{2\hat{p}_0(1-\hat{p}_0)} \overset{P}{\to}\sqrt{2p_0(1-p_0)}:=\sigma_0$, so by Slutzky's theorem, we get
$$
\frac{\sqrt{n}(\hat{p}_T-\hat{p}_C)}{\sigma_0}=\sqrt{\frac{n}{2}}\frac{\hat{p}_T-\hat{p}_C}{\sqrt{\hat{p}_0(1-\hat{p}_0)}}\overset{d}{\to}\mathcal{N}\left(\sqrt{n}\frac{p_T-p_C}{\sigma_0},\frac{\sigma_A^2}{\sigma_0^2}\right).
$$
Hence, for sufficiently large $n$, $U$ is approximately normal.

Note that under the null hypothesis, $\sigma_A^2=\sigma_0^2$ and $p_T=p_C$.
Thus, approximately, $U\sim \mathcal{N}(0,1)$ under $H_0$.

## `adoptr` and binomial endpoints

### Implementation details
Currently, `adoptr` only supports the specification of a single fixed reference value fo $p_C$,
while general prior distributions are supported for the effect size $\theta$. 

This is a limitation, as uncertainty about the control group rate cannot be
represented in this framework. However, in a trial comparing a new treatment to an
existing one, it is usually reasonable to assume that some information about
the event rate in the control group is available beforehand.

### Example
Assume we want to plan a two-armed trial with an assumed rate of events in the
control group of $p_C=0.3$. These parameters are encoded in the
`DataDistribution` object.

```{r}
datadist <- Binomial(0.3, two_armed = TRUE)
```

Let us furthermore postulate a normal prior distribution for $\theta$ with expectation $\mu=0.2$ and standard deviation $\sigma=0.2$, which was truncated to the interval $(-.29,0.69)$. It is necessary to use a truncation to ensure that $p_T \in (0,1)$.
```{r}
H_0        <- PointMassPrior(.0, 1)
prior      <- ContinuousPrior(function(x) 1/(pnorm(0.69,0.2,0.2)-pnorm(-0.29,0.2,0.2))*dnorm(x,0.2,0.2),
                              support = c(-0.29,0.69),
                              tighten_support = TRUE)
```

We require a maximal type one error of $\alpha\leq 0.025$ and a minimum expected power of $\mathbb{E}[1-\beta]\geq 0.8$. 

```{r}
alpha      <- 0.025
min_epower <- 0.8
toer_cnstr <- Power(datadist, H_0) <= alpha
epow_cnstr <- Power(datadist, condition(prior, c(0.0,0.69))) >= min_epower

```

Next, we need to choose an objective function, which will be the expected sample size under the chosen prior distribution for $\theta$ in this example. 
After having chosen a starting point for the optimization procedure, we use the `minimize` function to determine the optimal design parameters. 

```{r}
ess <- ExpectedSampleSize(datadist,prior)

init <- get_initial_design(0.2,0.025,0.2)

opt_design <- minimize(ess,subject_to(toer_cnstr,epow_cnstr),initial_design = init, check_constraints = TRUE)

plot(opt_design$design)
```

# Time-to-event endpoints
Time-to-event endpoints are another common type of endpoint used in clinical trials. Time-to-event data is two
dimensional and consists of an indicator denoting the occurrence of an event or censoring, and a time of the event or
censoring.

A common effect measure for time-to-event endpoints is the so called hazard-ratio, which is the ratio of
hazard functions between two groups or the ratio of the hazard of one group and a postulated baseline hazard.

In the following, the hazard ratio will be denoted by $\theta := \frac{\lambda_C(t)}{\lambda_T(t)}$, and we will assume that $\theta$ is constant over time.
Assuming that less hazard is favorable, the resulting hypotheses to be tested are

$$
H_0: \theta\leq 1 \quad \text{vs.} \quad H_1: \theta >1.
$$

Let $1,\dots,J$ be the distinct times of observed events in either group and let $n_{T,j}, n_{C,j}$ be the number of subjects, who neither had an event nor have been censored. Additionally, $O_{T,j},O_{C,j}$ are the observed number of events at time $j$. In the following we assume that there are no ties, so $O_{T,j},O_{C,j}\in \{0,1\}$. Define $n_j:=n_{T,j}+n_{C,j}$ and $O_j:= O_{T,j}+O_{C,j}$. Under the null, the hazard functions of the groups are equal. Thus, for $i \in \{T,C\}$, $O_{j}$ can be regarded as the number of events in a draw of size $n_{i,j}$, where the population has size $n_j$. This is exactly the definition of the hypergeometric distribution, so we know that $O_{i,j}\sim h(n_j,O_j,n_{i,j})$. This distribution has an expected value of $E_{i,j}:=\mathbb{E}[O_{i,j}]=n_{i,j}\frac{O_j}{n_j}$ and variance $V_{i,j}:=\text{Var}(O_{i,j})=E_{i,j}(\frac{n_j-O_j}{n_j})(\frac{n_j-n_{i,j}}{n_j-1})$. The log-rank test statistic is given by

$$
L=L_T:=\frac{\sum_{j=1}^J O_{T,j}-E_{T,j}}{\sqrt{\sum_{j=1}^J V_{T,j}}}
$$
Note that it does not matter whether we consider $L_T$ or $L_C$, both of them yield the same results, so we abbreviate $L_T$ to $L$.
By the central limit theorem, it is easy to see that $L \overset{d}{\to} \mathcal{N}(0,1)$. Under the alternative, it can be shown that $L \sim \mathcal{N}(\text{log}(\theta)\frac{1}{2}\sqrt{J},1)$ approximately. If $d$ is the combined probability of events (also known as event rate) in both arms, $J$ can be replaced by $2nd$ (where $n$ is, like in the whole chapter, the number of recruits per group). Thus, by estimating $d$ via a suitable estimator $\hat{d}$, we get an estimation of the total number of recruits for the study. 
In `adoptr`, the parameter $d$ needs to be pre-specified in order to obtain a one-parametric distribution (the other parameter is $\theta$), so $d$ is assumed to be fixed during the trial. This is similar to the case of binary endpoints, where the response rate in the control group $p_C$ had to be a constant value. Since the number of events is the main interest during a survival study, the need of a constant $d$ is not a strong limitation.

## Survival analysis and adaptive designs

### Notation

We expand the previous notation by an index $k\in \{1,2\}$, denoting the current stage. We observe $J_1$ events until the interim analysis, and we observe $J_2$ events until the final analysis. Note that $J_2$ is meant to be cumulative. Furthermore, let $O_{i,j,k}$ be the number of observed events in arm $i\in \{T,C\}$ at time $j \in \{1,\dots,J_k\}$ in stage $k \in \{1,2\}$. Like in the previous section, let $E_{i,j,k}= \mathbb{E}[O_{i,j,k}]=n_{i,j,k}\frac{O_{j,k}}{n_{j,k}}$, where $n_{i,j,k}$ is the number of patients that have not had an event until the end of stage $k$ and $O_{j,k}=O_{T,j,k}+O_{C,j,k}$ and $n_{j,k}=n_{T,j,k}+n_{C,j,k}$. In the same way, we can define $V_{i,j,k}$ like in the previous section. In this setting, we now have two test statistics $L_k$, $k \in \{1,2\}$, that can be calculated the same way like described above:

$$
L_k=\frac{\sum_{j=1}^{J_k} O_{T,j,k}-E_{T,j,k}}{\sqrt{\sum_{j=1}^{J_k} V_{T,j,k}}}
$$

### Dependence issues and solutions

In survival analysis, it is usual to have a certain follow-up time after the
accrual period in order to evaluate the number of events and when they occurred.
These recruits that have not had an event until the interim analysis
provide information for both stages: in the first stage, we know that these
subjects survived, and in the second stage, they may die or even survive until
the end of the trial. Thus, the test statistics $L_{1}$ from the interim analysis
and $L_{2}$ from the final analysis are not independent, even though this is an
important assumption in `adoptr` in order to calculate optimal designs.

#### Solution 1: Independent Increments

It can be shown that the following statistic

$$
Z_2:= \frac{\sqrt{J_2}L_2-\sqrt{J_1}L_1}{\sqrt{J_2-J_1}}
$$
is approximately standard normal under the null, $Z_2\sim \mathcal{N}(\text{log}(\theta)\frac{1}{2}\sqrt{J_2-J_1},1)$ and $L_1$ and $Z_2$ are approximately independent. Thus, $Z_2$ has the same distribution as the stage-wise (non-accumulative) test statistic of the second stage $L_{\Delta}$ and as `adoptr` assumes independence between the stage-wise test statistics, we can use $Z_2$ as a replacement of $L_{\Delta}$. 

In the following, we denote the in-built `adoptr`functions to get the sample sizes by $\texttt{n1}$ and $\texttt{n2}$.

The procedure is now as follows:

During the accrual time, recruit approximately $\frac{\texttt{n1}}{d}$ patients per group and conduct the interim analysis after $2\cdot \texttt{n_1}$ events have been observed overall. Assume, $q$ patients have not had an event until the interim analysis. These patients are censored for the computation of $L_1$. 

After the interim analysis, we have $2\cdot \texttt{n2}(L_1)$ events that should be ovserved during the second stage. But of course, $q$ patients are still in the trial, so we only need to recruit $\frac{2\cdot \texttt{n2}(L_1)}{d}-q$ new people.

When the required number of events have been observed in the second stage, we now conduct the final analysis using $Z_2$ instead of $L_{\Delta}$.


#### Solution 2: Left Truncation at the Second Stage

Another approach uses left truncation and right censoring. In the following, $R_i$ stands for the calendar time of entry for individual $i$. Furthermore, let $T_i$ be its time from entry to the event and $C_i$ its time from entry to censoring. Assume that these values are independent and let $r_i,t_i,c_i$ be realizations of $R_i,T_i$ and $C_i$. Additionally, we denote the calendar time of the interim analysis by $t_{Int}$ and the calender time of the final analysis by $t_{Fin}$. 

In the following, it is important to remember $n_{j,k}$ which was defined to be the cardinality of the risk set, which is the number of patients at risk at time $j$ in stage $k$. This value is important for the calculation of the log-rank test statistics. 

Let $i$ be recruited before the interim analysis. Now define $y_{i,1}:=\min \{t_i,c_i,t_{Int}\}$, so $y_{i,1}$ can be interpreted as the minimum time until "something" happens to individual $i$ in the first stage (either the interim analysis is conducted, $i$ had an event or it was censored). The risk interval is now defined to be $(0,y_{i,1})$, which means that that individual $i$ belongs to the risk set at the event time $t_j$ if $\min \{c_i,t_{Int}-r_i\}\geq x_j$. Therefore, the first stage analysis is done in the usual way.

If $i$ has not had an event yet or was recruited in the second stage, we define $y_{i,2}:=\min\{t_i,c_i,t_{Fin}\}$. As before, $y_{i,2}$ can be interpreted as the minimum time until something happens to individual $i$ in the second stage: Either the final analysis is conducted, $i$ had an event or it was censored. As the second test statistic is cumulative, one should expect the risk interval to be $(0,y_{i,2})$, but actually, it is defined as $(t_{Int}-r_i,y_{i,2})$. This is due to the fact that a patient $j$ which was recruited in the first stage, but had no event until the interim analysis, has already provided the information of "no event" in the time span $(t_{Int}-r_i,t_{Int})$ for the interim. To avoid double-counting in the second analysis, we say that this patient is not in the risk set until this time span is over. 

Actually, it can be shown that the resulting test statistics $L_1$ and $L_{Trunc}$ (the statistic of the final analysis) are independent, so methods from `adoptr` can be applied.

One may conduct the trial according to the following instructions:

For the interim, the procedure is the same as for the independent increment solution, but it is important to know the time span the subjects have been in the trial until the analysis is conducted.

After the interim analysis, we have $2\cdot \texttt{n2}(L_1)$ events that should be observed during the second stage. But under the assumption that $q$ patients have not had an event yet, we only need to recruit $\frac{2\cdot \texttt{n2}(L_1)}{d}-q$ new people. 

The test statistic of the final analysis $L_{Trunc}$ is not computed in the usual way, but $n_{j,k}$ now depends on whether a subject has had an event at the interim analysis, so recruit $i$ is in the risk set if $t_{Int}-r_i < j \leq y_{i,2}$. After having calculated $L_{Trunc}$ this way, we can now use this test statistic instead of $L_\Delta$ for the final analysis.


#### Example
In `adoptr`, trial designs to investigate time-to-event endpoints can be
represented in objects of the class `SurvivalDesign`.
These designs consist of first stage futility and efficacy boundary
$c_f$ and $c_e$, the number of events to observe until the conduction of
the interim analysis $n_1$ (or $2\cdot n_1}$ for two-armed trials), the
second stage efficacy boundary $c_2$, the number of events to be observed
in the second stage $n_2$ (or $2 \cdot n_2$ for two-armed trials),
and the postulated rate of events $d$.

Let us say we want to plan a two-armed trial trial, where we assume
an average rate of events of $d=0.7$.

```{r}
datadist <- Survival(0.7, two_armed = TRUE)
```

The postulated rate of events $d$ is saved in the `DataDistribution` object and
will be handed down to any design optimized with respect to this distribution.
It will be used to convert the required number of events $n_2$ to the estimated
number of required recruits $N_2$ via $N_2 = n_2 / d$, but it has no other uses.
All other design parameters are invariant with respect to the choice of $d$.

Effect sizes for time-to-event trials are formulated with respect to a
hazard ratio. For our example, we assume $\theta=1$ for the null hypothesis,
and a point alternative hypothesis of $\theta=1.7$.

```{r}
H_0 <- PointMassPrior(1,1)
H_1 <- PointMassPrior(1.7,1)
```

Our desired design should have a maximal type I error $\alpha\leq 0.025$ and a minimum power of $(1-\beta)\geq 0.8$.

```{r}
alpha <- 0.025
min_power <- 0.8
toer_con <- Power(datadist,H_0)<=alpha
pow_con <- Power(datadist,H_1)>=min_power
```


```{r}
exp_no_events <- ExpectedNumberOfEvents(datadist,H_1)
init <- get_initial_design(1.7,0.025,0.2,dist=datadist)
opt_survival <- minimize(exp_no_events,subject_to(toer_con,pow_con),initial_design = init,check_constraints=TRUE)

summary(opt_survival$design)
```


It should be noted that the number of required events $n_1$ and $n_2$ are the
important information thresholds to be adhered to for testing purposes, while $N_1$ and
$N_2$ just serve as rough estimates for the required number of subjects to reach
these milestones. If additional information about e.g. time-dependence of
baseline hazards is available, it may be worthwhile to use a more sophisticated
approach than the $N_2 = n_2 / d$ formula to estimate the number of required subjects.

